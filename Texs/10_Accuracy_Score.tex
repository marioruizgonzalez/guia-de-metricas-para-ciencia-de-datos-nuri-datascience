\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\geometry{margin=1in}

\title{Guía de Métricas para Ciencia de Datos}
\author{Mario Ruiz}
\date{Noviembre 2025}

\begin{document}

% \maketitle

\section*{Accuracy Score}

\subsection*{Fórmula base}
\[
\text{Accuracy} = \frac{\text{Número de predicciones correctas}}{\text{Número total de predicciones}}
\]

\subsection*{Representación matemática formal}
\noindent
Para clasificación binaria:
\[
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\]

\noindent
Para clasificación multiclase:
\[
\text{Accuracy} = \frac{\sum_{i=1}^{k} \text{Predicciones correctas por clase}_i}{N_{\text{total}}}
\]

\noindent
De forma general:
\[
\text{Accuracy} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I}(\hat{y}_i = y_i)
\]

\noindent
Donde:
\begin{itemize}
    \item $TP$: verdaderos positivos
    \item $TN$: verdaderos negativos
    \item $FP$: falsos positivos
    \item $FN$: falsos negativos
    \item $\mathbb{I}(\hat{y}_i = y_i)$: función indicadora que vale 1 si la predicción coincide con la etiqueta real y 0 en caso contrario
    \item $N$: número total de observaciones
\end{itemize}

\subsection*{Fundamento matemático}
El \textbf{Accuracy Score} estima la probabilidad empírica de que una predicción sea correcta bajo la distribución observada de los datos:
\[
P(\hat{y} = y) \approx \frac{1}{N} \sum_{i=1}^{N} \mathbb{I}(\hat{y}_i = y_i)
\]
Esta expresión convierte el conteo discreto de aciertos en una probabilidad promedio, permitiendo una interpretación directa como porcentaje de aciertos.

\subsection*{Propiedades matemáticas}
\begin{itemize}
    \item \textbf{Rango definido:} $[0, 1]$, donde 0 indica clasificación totalmente incorrecta y 1 predicciones perfectas.
    \item \textbf{Monotonicidad:} Un mayor valor de accuracy implica un mejor desempeño general del modelo.
    \item \textbf{Interpretabilidad:} Corresponde directamente a la proporción de aciertos, fácil de comunicar e interpretar.
    \item \textbf{Eficiencia computacional:} Se calcula en tiempo lineal $O(n)$ y requiere espacio constante $O(1)$.
\end{itemize}

\subsection*{Limitación en datos desbalanceados}
En conjuntos de datos con clases desbalanceadas, la métrica puede inducir a error:
\[
\text{Accuracy alto} \; \centernot\Rightarrow \; \text{buen modelo}
\]
Si una clase domina el conjunto, un modelo que siempre predice la clase mayoritaria puede obtener un accuracy elevado sin aprender realmente el patrón de clasificación.

\subsection*{Resumen}
El \textit{Accuracy Score} mide la proporción de aciertos totales del modelo.  
Su simplicidad e interpretabilidad la convierten en una métrica estándar para tareas de clasificación balanceadas, aunque requiere complementarse con métricas adicionales (Precision, Recall, F1) cuando las clases no están equilibradas.

\end{document}
