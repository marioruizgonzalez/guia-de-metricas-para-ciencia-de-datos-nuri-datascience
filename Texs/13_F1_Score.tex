\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\geometry{margin=1in}

\title{Guía de Métricas para Ciencia de Datos}
\author{Mario Ruiz}
\date{Noviembre 2025}

\begin{document}

% \maketitle

\section*{F1 Score}

\subsection*{Fórmula principal}
\[
F_1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\]

\subsection*{Componentes fundamentales}

\noindent
El F1 Score combina las dos métricas esenciales de la clasificación: \textbf{Precision} y \textbf{Recall}.  
Para comprenderlo, partimos de la matriz de confusión:

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
 & \textbf{Predicción Positiva} & \textbf{Predicción Negativa} \\
\hline
\textbf{Real Positiva} & TP & FN \\
\hline
\textbf{Real Negativa} & FP & TN \\
\hline
\end{tabular}
\end{center}

\noindent
Donde:
\begin{itemize}
    \item $TP$: verdaderos positivos — casos positivos correctamente identificados.
    \item $FP$: falsos positivos — casos negativos clasificados como positivos.
    \item $FN$: falsos negativos — casos positivos clasificados como negativos.
    \item $TN$: verdaderos negativos — casos negativos correctamente identificados.
\end{itemize}

\noindent
\textbf{Precision:}
\[
\text{Precision} = \frac{TP}{TP + FP}
\]
“De todas las predicciones positivas, ¿cuántas fueron correctas?”

\noindent
\textbf{Recall (Sensibilidad):}
\[
\text{Recall} = \frac{TP}{TP + FN}
\]
“De todos los casos realmente positivos, ¿cuántos detectó el modelo?”

\subsection*{¿Por qué utiliza la media armónica?}
El F1 Score emplea la \textbf{media armónica} en lugar de la aritmética:
\[
\text{Media aritmética} = \frac{\text{Precision} + \text{Recall}}{2}, 
\quad
\text{Media armónica} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\]

\noindent
La media armónica penaliza fuertemente los desequilibrios entre ambas métricas.  
Si una es muy baja, el F1 refleja correctamente un rendimiento deficiente.

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Precision} & \textbf{Recall} & \textbf{Resultado} \\
\hline
0.9 & 0.9 & F1 = 0.9 \\
0.9 & 0.1 & F1 = 0.18 \\
\hline
\end{tabular}
\end{center}

\noindent
En este ejemplo, la media aritmética sería 0.5, ocultando el pobre desempeño;  
el F1, en cambio, lo penaliza apropiadamente.

\subsection*{Propiedades matemáticas}
\begin{itemize}
    \item \textbf{Rango:} $F_1 \in [0,1]$, donde 1 indica desempeño perfecto.
    \item \textbf{Simetría:} da igual peso a la Precision y al Recall.
    \item \textbf{Monotonicidad:} mejora si cualquiera de las métricas aumenta, manteniendo la otra constante.
    \item \textbf{Penalización:} siempre se cumple $F_1 \leq \min(\text{Precision}, \text{Recall})$.
\end{itemize}

\subsection*{Resumen}
El \textit{F1 Score} representa un balance matemático entre la exactitud de las predicciones positivas (Precision) y la cobertura de los casos reales (Recall).  
Es especialmente útil en contextos con clases desbalanceadas, donde refleja de manera más justa la capacidad general del modelo que la simple exactitud (Accuracy).

\end{document}
