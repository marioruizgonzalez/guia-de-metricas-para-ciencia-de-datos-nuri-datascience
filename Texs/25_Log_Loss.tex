\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\geometry{margin=1in}

\title{Guía de Métricas para Ciencia de Datos}
\author{Mario Ruiz}
\date{Noviembre 2025}

\begin{document}

% \maketitle

\section*{Log Loss (Cross-Entropy Loss)}

\subsection*{Definición general}
La \textbf{Log Loss}, también conocida como \textit{Cross-Entropy Loss}, mide la incertidumbre promedio en las predicciones probabilísticas de un modelo de clasificación.  
Evalúa qué tan cercanas están las probabilidades predichas a las etiquetas verdaderas.

\subsection*{Fórmula para clasificación binaria}
\[
\text{Log Loss} = -\frac{1}{N} \sum_{i=1}^{N} [y_i \log(p_i) + (1 - y_i)\log(1 - p_i)]
\]
\noindent
Donde:
\begin{itemize}
    \item $N$: Número total de observaciones.
    \item $y_i$: Etiqueta verdadera (0 o 1) para la observación $i$.
    \item $p_i$: Probabilidad predicha de la clase positiva para la observación $i$.
\end{itemize}

\subsection*{Fórmula para clasificación multiclase}
\[
\text{Log Loss} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{M} y_{ij} \log(p_{ij})
\]
\noindent
Donde:
\begin{itemize}
    \item $M$: Número total de clases.
    \item $y_{ij}$: Indicador binario, $1$ si la observación $i$ pertenece a la clase $j$, $0$ en caso contrario.
    \item $p_{ij}$: Probabilidad predicha de que la observación $i$ pertenezca a la clase $j$.
\end{itemize}

\subsection*{Desglose conceptual}
\begin{enumerate}
    \item \textbf{Logaritmo natural:}  
    $\log(p_i)$ es negativo cuando $0 < p_i < 1$ y tiende a $-\infty$ cuando $p_i \to 0$.  
    Penaliza fuertemente las predicciones erróneas con alta confianza.
    \item \textbf{Signo negativo:}  
    Asegura que la métrica sea positiva y que menores valores indiquen mejor desempeño.
    \item \textbf{Suma ponderada:}  
    Solo contribuye el término correspondiente a la clase verdadera, ya que $y_i$ es cero para las demás clases.
\end{enumerate}

\subsection*{Propiedades matemáticas clave}

\paragraph{1. Penalización exponencial}
Cuando $p_i$ es muy pequeño para la clase correcta, $\log(p_i)$ se aproxima a $-\infty$, generando una penalización severa.  
Esto desalienta predicciones erróneas con alta confianza.

\paragraph{2. Convexidad}
La función de Log Loss es convexa respecto a las probabilidades $p_i$, garantizando que los algoritmos de optimización puedan encontrar un mínimo global estable durante el entrenamiento.

\paragraph{3. Diferenciabilidad}
Es continua y diferenciable en todo su dominio, lo que permite el uso de métodos de gradiente y facilita la retropropagación en redes neuronales.

\subsection*{Interpretación numérica}
Ejemplo: para una observación donde la clase verdadera es $1$.

\begin{center}
\begin{tabular}{|c|c|l|}
\hline
\textbf{Probabilidad predicha} & \textbf{Log Loss individual} & \textbf{Interpretación} \\
\hline
0.99 & 0.01 & Excelente: muy confiado y correcto \\
0.70 & 0.36 & Bueno: confiado y correcto \\
0.51 & 0.67 & Aceptable: baja confianza, pero correcto \\
0.50 & 0.69 & Neutral: completamente incierto \\
0.30 & 1.20 & Malo: confiado pero incorrecto \\
0.01 & 4.61 & Terrible: muy confiado y muy incorrecto \\
\hline
\end{tabular}
\end{center}

\subsection*{Rango e interpretación}
\begin{itemize}
    \item Rango: $[0, +\infty)$
    \item $\text{Log Loss} = 0$: predicciones perfectas.
    \item Valores grandes indican predicciones con alta confianza errónea.
\end{itemize}

\subsection*{Conclusión}
La \textbf{Log Loss} ofrece una evaluación más sensible que el accuracy, especialmente en modelos probabilísticos.  
Matemáticamente, mide la entropía cruzada entre la distribución real y la predicha, proporcionando una interpretación directa del grado de incertidumbre y calibración del modelo.

\end{document}
